{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import evaluation\n",
    "import make_prediction as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('../data/training.csv', parse_dates= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an external set - DO NOT CHANGE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prediction_set(df):\n",
    "    external_set=df.sample(frac=0.1,random_state=42)\n",
    "    external_set['row_id']=external_set.reset_index().index\n",
    "    train_set=df.drop(external_set.index)\n",
    "    return train_set, external_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set,external_set=create_prediction_set(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(395535, 49)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "external_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating difference features \n",
    "\n",
    "#sample['pct_change']=sample['sensor_t4'].diff()\n",
    "\n",
    "def pct_change(variable,df):\n",
    "    for var in variable:\n",
    "        if var in df.columns:\n",
    "            df[var+'_pct_change']=df[var].pct_change()\n",
    "        else:\n",
    "            print('Column does not exist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "slist =['sensor_t4','sensor_t1','sensor_t5','sensor_q1','sensor_s4','sensor_t6','sensor_q2',\n",
    "       'sensor_t9','sensor_s3','sensor_t2','sensor_t7','sensor_s1','sensor_t3','sensor_s2','sensor_t8']\n",
    "\n",
    "s=pct_change(slist,train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run for holdout st \n",
    "t=pct_change(slist,external_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(395535, 49)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "external_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some products are missing process \n",
    "#### Like s50016128 , this product has only gone throught process B and C "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std(x):\n",
    "    f=np.std(x)\n",
    "    return f\n",
    "\n",
    "def mean(x):\n",
    "    f=np.mean(x)\n",
    "    return f\n",
    "\n",
    "def entropy(s):\n",
    "    px = s.value_counts() / s.shape[0]\n",
    "    lpx = np.log2(px)\n",
    "    ent = -1.0*(px*lpx).sum()\n",
    "    return ent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Numerical data from sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregrations ={\n",
    "                'sensor_t4':[min,max,mean,std,entropy],\n",
    "                'sensor_t1':[min,max,mean,std,entropy],\n",
    "                'sensor_t5':[min,max,mean,std,entropy],\n",
    "                'sensor_q1':[min,max,mean,std,entropy],\n",
    "                'sensor_s4':[min,max,mean,std,entropy],\n",
    "                'sensor_t6':[min,max,mean,std,entropy],\n",
    "                'sensor_s3':[min,max,mean,std,entropy],\n",
    "                'sensor_q2':[min,max,mean,std,entropy],\n",
    "                'sensor_t9':[min,max,mean,std,entropy],\n",
    "                'sensor_t2':[min,max,mean,std,entropy],\n",
    "                'sensor_s3':[min,max,mean,std,entropy],\n",
    "                'sensor_t7':[min,max,mean,std,entropy],\n",
    "                'sensor_s2':[min,max,mean,std,entropy],\n",
    "                'sensor_t1':[min,max,mean,std,entropy],\n",
    "                'sensor_t3':[min,max,mean,std,entropy],\n",
    "                'sensor_s2':[min,max,mean,std,entropy],\n",
    "                'sensor_t8':[min,max,mean,std,entropy],\n",
    "                'timestamp':[min,max],\n",
    "                'flow_id':['last'],\n",
    "                'lot_id':['last'],\n",
    "                'flag_b2':[entropy,mean],\n",
    "                'flag_c2':[entropy,mean],\n",
    "                'flag_e': [entropy,mean],\n",
    "                'flag_a2':[entropy,mean],\n",
    "                'flag_a4':[entropy,mean],\n",
    "                'flag_b1':[entropy,mean],\n",
    "                'flag_d': [entropy,mean],\n",
    "                'flag_a1':[entropy,mean],\n",
    "                'flag_b3':[entropy,mean],\n",
    "                'flag_b4':[entropy,mean],\n",
    "                'flag_c1':[entropy,mean],\n",
    "                'flag_a3':[entropy,mean],\n",
    "                'flag_a5':[entropy,mean],\n",
    "                'flag_b2':['last',mean],\n",
    "                'flag_c2':['last',mean],\n",
    "                'flag_e':['last',mean],\n",
    "                'flag_a2':['last',mean],\n",
    "                'flag_a4':['last',mean],\n",
    "                'flag_b1':['last',mean],\n",
    "                'flag_d':['last',mean],\n",
    "                'flag_a1':['last',mean],\n",
    "                'flag_b3':['last',mean],\n",
    "                'flag_b4':['last',mean],\n",
    "                'flag_c1':['last',mean],\n",
    "                'flag_a3':['last',mean],\n",
    "                'flag_a5':['last',mean],\n",
    "                'sensor_t4_pct_change':[min,max],\n",
    "                'sensor_t1_pct_change':[min,max],\n",
    "                'sensor_t5_pct_change':[min,max],\n",
    "                'sensor_q1_pct_change':[min,max],\n",
    "                'sensor_s4_pct_change':[min,max],\n",
    "                'sensor_t6_pct_change':[min,max],\n",
    "                'sensor_s3_pct_change':[min,max],\n",
    "                'sensor_q2_pct_change':[min,max],\n",
    "                'sensor_t9_pct_change':[min,max],\n",
    "                'sensor_t2_pct_change':[min,max],\n",
    "                'sensor_s3_pct_change':[min,max],\n",
    "                'sensor_t7_pct_change':[min,max],\n",
    "                'sensor_s2_pct_change':[min,max],\n",
    "                'sensor_t1_pct_change':[min,max],\n",
    "                'sensor_t3_pct_change':[min,max],\n",
    "                'sensor_s2_pct_change':[min,max],\n",
    "                'sensor_t8_pct_change':[min,max]\n",
    "                \n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(train_set['process'],density=True, facecolor='g', alpha=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_obj=train_set.groupby(['product_id','process'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# external dataset HOLDOUT \n",
    "\n",
    "group_obj_ext=external_set.groupby(['product_id','process'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample\n",
    "sample_group = sample.groupby(['product_id','process'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply aggregation functions\n",
    "'''change the groupby object to full df'''\n",
    "f=group_obj_ext.agg(aggregrations)\n",
    "\n",
    "# renaming columns with multiIndex levels \n",
    "f.columns = [\"_\".join(x) for x in f.columns.ravel()]\n",
    "#unstacking the agg features to create separate feature for each process\n",
    "funstack=f.unstack(level=1)\n",
    "# Renaming these features \n",
    "funstack.columns = [\"_\".join(x) for x in funstack.columns.ravel()]\n",
    "#flattening the index \n",
    "funstack.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat= funstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4404, 513)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### adding time duration \n",
    "##### count total seconds spent on each process \n",
    "######tot_seconds_proccessA = 290\n",
    "######tot_seconds_proccessB = 456\n",
    "sample['timestamp_seconds']=sample.loc['timestamp'].dt.second.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting string to datetime to calculate time spent\n",
    "timestamplist =['timestamp_max_A','timestamp_min_A','timestamp_max_B','timestamp_min_B','timestamp_max_C',\n",
    "                 'timestamp_min_C','timestamp_max_D','timestamp_min_D']\n",
    "\n",
    "# Fill the Missing timestamps as 0 \n",
    "## Assumption: If no time was spent on the stage then it is meaningful 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funstack.loc[:,timestamplist].apply(pd.to_datetime)\n",
    "for timecolumn in timestamplist:\n",
    "    flat[timecolumn]= flat[timecolumn].apply(lambda x : pd.to_datetime(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the Missing timestamps as 0 \n",
    "## Assumption: If no time was spent on the stage then it is meaningful 0\n",
    "flat['timeSpent_A'] = (flat['timestamp_max_A'] - flat['timestamp_min_A']).fillna(pd.Timedelta(seconds=0))/np.timedelta64(1, 's')\n",
    "flat['timeSpent_B'] = (flat['timestamp_max_B'] - flat['timestamp_min_B']).fillna(pd.Timedelta(seconds=0))/np.timedelta64(1, 's')\n",
    "flat['timeSpent_C'] = (flat['timestamp_max_C'] - flat['timestamp_min_C']).fillna(pd.Timedelta(seconds=0))/np.timedelta64(1, 's')\n",
    "flat['timeSpent_D'] = (flat['timestamp_max_D'] - flat['timestamp_min_D']).fillna(pd.Timedelta(seconds=0))/np.timedelta64(1, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining the Target label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv('../data/training_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged= flat.merge(labels, left_on='product_id', right_on='product_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log transformation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged['log_target']= np.log(merged['qc_reading'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataRobot - Project building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datarobot as dr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<datarobot.rest.RESTClientObject at 0x16b63f7f0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dr.Client(token='NWVlYWVmNzI4YjcxY2IyODE2ZTJhOTg5OnhSdjlqSEZ2NlU5cHc4WWQwZlh1RHNkbGZGREE0dlQzQnB5L3k2V2d3a2c9', endpoint='https://app.datarobot.com/api/v2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID: 5f2510f06c6a7a108d8d88ae\n"
     ]
    }
   ],
   "source": [
    "project = dr.Project.start(sourcedata=merged,\n",
    "                           target='qc_reading',\n",
    "                           project_name='Exec_train_pred_v10',worker_count=-1)\n",
    "print('Project ID: {}'.format(project.id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare the external holdout set \n",
    "\n",
    "run the preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_data= merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the project\n",
    "proj=dr.Project.get('5f2533716c6a7a14d6aff706')\n",
    "# chose the model \n",
    "model = dr.Model.get(project=proj.id,\n",
    "                     model_id='5f253a83984a4326a8d7f66c')\n",
    "#upload the external dataset \n",
    "#external= proj.upload_dataset(holdout_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_id=proj.id\n",
    "model_id=model.id\n",
    "ext_id= external.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_(project_id,model_id,ext_id):\n",
    "    #upload external set\n",
    "    ## Check if there is an external set already. If yes, then get the dataset.id instead of uploading a new one \n",
    "    model = dr.Model.get(project=proj_id,\n",
    "                     model_id=model_id)\n",
    "    predict_job=model.request_predictions(ext_id)\n",
    "    predict_job.wait_for_completion()\n",
    "    #get predictions\n",
    "    predictions = predict_job.get_result_when_complete()\n",
    "    predictions['exp_predictions'] = np.exp(predictions['prediction'])\n",
    "    predictions['row_id']= predictions.reset_index().index\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_results(predictions,actual_df):\n",
    "    result=actual_df.merge(predictions,left_on='row_id',right_on='row_id', how='left')\n",
    "    labels = pd.read_csv('../data/training_label.csv')\n",
    "    holdout_with_target= result.merge(labels, left_on='product_id', right_on='product_id', how='left')\n",
    "    return amape(holdout_with_target['qc_reading'],holdout_with_target['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "r =get_predictions_(proj_id,model_id,ext_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6696"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = join_results(r,external_set)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions['exp_predictions'] = np.exp(predictions['prediction'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation \n",
    "\n",
    "\n",
    "#### Beat 0.121 score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amape(actual,pred):\n",
    "    #actual, pred = np.array(actual), np.array(pred)\n",
    "    for a in actual:\n",
    "        if a < 2000000:\n",
    "            actual[a] = 2000000\n",
    "            #mape =np.round(np.mean(np.abs((actual - pred) / actual)),4)\n",
    "        else:\n",
    "            None\n",
    "            mape =np.round(np.mean(np.abs((actual - pred) / actual)),4)\n",
    "            return mape\n",
    "\n",
    "#def mape(actual, pred): \n",
    " #   actual, pred = np.array(actual), np.array(pred)\n",
    "  #  return np.mean(np.abs((actual - pred) / actual)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
